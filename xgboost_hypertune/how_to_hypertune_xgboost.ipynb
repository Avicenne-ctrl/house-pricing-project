{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create project fine tuning hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the project folder create a trainer folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python fine tuning script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create in trainer your task.py fil in which we want to fine tune hyperparameters for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/task.py\n",
    "#first line creates the file in the trainer folder\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "import argparse\n",
    "import hypertune\n",
    "\n",
    "def get_args_xgboost():\n",
    "    \"\"\"\n",
    "        Function that will takes params from VertexAi configuration\n",
    "\n",
    "        Returns:\n",
    "            args (ArgumentParser): \n",
    "                corresponding params for xgboost\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--learning_rate', required=True, type=float, help='learning rate')\n",
    "    parser.add_argument(\n",
    "        '--n_estimators', required=True, type=int, help='n_estimators')\n",
    "    parser.add_argument(\n",
    "        '--max_depth', required=True, type=int, help='max_depth')\n",
    "    parser.add_argument(\n",
    "        '--subsample', required=True, type=float, help='subsample')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    return args\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data(\n",
    "        path=\"boston_housing.npz\", test_split=0.2, seed=113\n",
    "    )\n",
    "\n",
    "    columns = [\n",
    "        \"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \n",
    "        \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\"\n",
    "    ]\n",
    "\n",
    "    # Convertir les ensembles de donn√©es en DataFrames pandas\n",
    "    return pd.DataFrame(x_train, columns=columns), pd.Series(y_train), pd.DataFrame(x_test, columns=columns), pd.Series(y_test)\n",
    "\n",
    "\n",
    "def create_xgboost(n_estimators: int, max_depth: int, learning_rate: float, subsample: float):\n",
    "    \"\"\"\n",
    "        init the xgboost regressor model with hyperparameters\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "            n_estimators (int):\n",
    "            max_depth (int):\n",
    "            learning_rate (float):\n",
    "            subsample (float):\n",
    "            \n",
    "        Returns:\n",
    "        \n",
    "            xgb (XGBRegressor):\n",
    "                the model with custom params\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    xgb = XGBRegressor(n_estimators  = n_estimators, \n",
    "                       max_depth     =max_depth, \n",
    "                       learning_rate =learning_rate, \n",
    "                       subsample     =subsample)\n",
    "    \n",
    "    return xgb\n",
    "    \n",
    "    \n",
    "def create_rfr(n_estimators: int, max_depth: int, min_samples_split: int, min_samples_leaf: int):\n",
    "    \"\"\"\n",
    "        Init RandomForest model with corresponding params\n",
    "        \n",
    "        Args:\n",
    "            n_estimators (int):\n",
    "            \n",
    "            max_depth (int):\n",
    "            \n",
    "            learning_rate (float):\n",
    "            \n",
    "            subsample (float):\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "            rfr (RandomForestRegressor):\n",
    "                the model with custom params\n",
    "        \n",
    "    \"\"\"\n",
    "    rfr = RandomForestRegressor(n_estimators      = n_estimators, \n",
    "                                max_depth         = max_depth, \n",
    "                                min_samples_split = min_samples_split, \n",
    "                                min_samples_leaf  = min_samples_leaf)\n",
    "    \n",
    "    return rfr\n",
    "\n",
    "\n",
    "\n",
    "args_xgb = get_args_xgboost()\n",
    "\n",
    "x_train, y_train, x_val, y_val = load_data()\n",
    "\n",
    "with strategy.scope():\n",
    "    xgb = create_xgboost(n_estimators  = args_xgb.n_estimators, \n",
    "                            max_depth     = args_xgb.max_depth, \n",
    "                            learning_rate = args_xgb.learning_rate, \n",
    "                            subsample     = args_xgb.subsample)  \n",
    "\n",
    "xgb.fit(x_train, y_train)\n",
    "\n",
    "pred_xgb = xgb.predict(x_val)\n",
    "\n",
    "hpt = hypertune.HyperTune()\n",
    "\n",
    "hpt.report_hyperparameter_tuning_metric(\n",
    "    hyperparameter_metric_tag='r2_score',\n",
    "    metric_value=r2_score(y_val, pred_xgb),)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will need to create a Dockerfile in order to create the container and send it to Google Cloud (Artifact Registry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-11\n",
    "\n",
    "# Avoiding crash due to scikit-learn and xgboost installation \n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    build-essential \\\n",
    "    libatlas-base-dev \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "ENV APP_HOME /app\n",
    "WORKDIR $APP_HOME\n",
    "COPY . ./\n",
    "\n",
    "RUN pip install -r requirements.txt\n",
    "CMD [\"python\", \"app.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have installed Google Cloud SDK and added to the shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### connect to your project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=HNn13Spn42tingQCXKOC6KIarfVyM3&access_type=offline&code_challenge=GouxY7FVODQJvlTF_JC11BQkguB8LlxT9X0MNVy77LM&code_challenge_method=S256\n",
      "\n",
      "\n",
      "Credentials saved to file: [/Users/avicenne/.config/gcloud/application_default_credentials.json]\n",
      "\n",
      "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
      "\n",
      "Quota project \"trans-sunset-439207-f2\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n",
      "\n",
      "\n",
      "Updates are available for some Google Cloud CLI components.  To install them,\n",
      "please run:\n",
      "  $ gcloud components update\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"trans-sunset-439207-f2\"\n",
    "BUCKET_URI = f\"gs://{PROJECT_ID}-boston-house-pricing\" \n",
    "LOCATION = \"us-central1\"\n",
    "IMAGE_NAME = \"xgboost-hypertune\"\n",
    "IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "# Select project\n",
    "!gcloud config set project $PROJECT_ID\n",
    "\n",
    "# Check if bucket already exist\n",
    "!gsutil ls $BUCKET_URI\n",
    "\n",
    "# If not create it\n",
    "! gsutil mb -l {LOCATION} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/0)  docker:desktop-linux\n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                    docker:desktop-linux\n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (1/2)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 391B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for gcr.io/deeplearning-platform-release/tf2  0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.4s (1/2)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 391B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for gcr.io/deeplearning-platform-release/tf2  0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (1/2)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 391B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for gcr.io/deeplearning-platform-release/tf2  0.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (2/2)                                    docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 391B                                       0.0s\n",
      "\u001b[0m\u001b[31m => ERROR [internal] load metadata for gcr.io/deeplearning-platform-relea  0.5s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.7s (2/2) FINISHED                           docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 391B                                       0.0s\n",
      "\u001b[0m\u001b[31m => ERROR [internal] load metadata for gcr.io/deeplearning-platform-relea  0.5s\n",
      "\u001b[0m\u001b[?25h------\n",
      " > [internal] load metadata for gcr.io/deeplearning-platform-release/tf2-gpu.2-7:latest:\n",
      "------\n",
      "Dockerfile:2\n",
      "--------------------\n",
      "   1 |     \n",
      "   2 | >>> FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-7\n",
      "   3 |     \n",
      "   4 |     # Avoiding crash due to scikit-learn and xgboost installation \n",
      "--------------------\n",
      "ERROR: failed to solve: gcr.io/deeplearning-platform-release/tf2-gpu.2-7: failed to resolve source metadata for gcr.io/deeplearning-platform-release/tf2-gpu.2-7:latest: gcr.io/deeplearning-platform-release/tf2-gpu.2-7:latest: not found\n"
     ]
    }
   ],
   "source": [
    "!docker build ./ -t $IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertex Ai fine tuning CustomJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\"image_uri\": f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}\"},\n",
    "    }\n",
    "]\n",
    "\n",
    "metric_spec = {\"r2_score\": \"maximize\"}\n",
    "\n",
    "parameter_spec = {\n",
    "    \"learning_rate\": hpt.DoubleParameterSpec(min=0.4, max=0.8, scale=\"log\"),\n",
    "    \"n_estimators\": hpt.IntegerParameterSpec(min=50, max=100, scale=\"linear\"),\n",
    "    \"max_depth\": hpt.IntegerParameterSpec(min=5, max=10, scale=\"Linear\"),\n",
    "    \"subsample\": hpt.DoubleParameterSpec(min=0.4, max=0.8, scale=\"log\"),\n",
    "}\n",
    "\n",
    "JOB_NAME = \"house-pricing-hyperparam-job\"\n",
    "\n",
    "my_custom_job = aiplatform.CustomJob(\n",
    "    display_name=JOB_NAME,\n",
    "    project=PROJECT_ID,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=BUCKET_URI,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run HyperparameterTuningJob\n",
    "\n",
    "hp_job = aiplatform.HyperparameterTuningJob(\n",
    "    display_name=JOB_NAME,\n",
    "    custom_job=my_custom_job,\n",
    "    metric_spec=metric_spec,\n",
    "    parameter_spec=parameter_spec,\n",
    "    max_trial_count=15,\n",
    "    parallel_trial_count=3,\n",
    ")\n",
    "\n",
    "hp_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpt_job = aiplatform.HyperparameterTuningJob.get(\n",
    "        resource_name=JOB_NAME,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
